#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å®Œæ•´çš„ZOLå†…å­˜æ•°æ®æŠ“å–å’Œå­˜å‚¨è„šæœ¬
æŠ“å–æ‰€æœ‰å“ç‰Œã€æ‰€æœ‰è§„æ ¼çš„å†…å­˜äº§å“å¹¶ä¿å­˜åˆ°æ•°æ®åº“
æ”¯æŒå•çº¿ç¨‹å’Œå¹¶å‘æ¨¡å¼
"""

import sys
import os
import time
import argparse
from datetime import datetime
from typing import List, Dict, Any

# Set UTF-8 encoding
if hasattr(sys.stdout, 'reconfigure'):
    sys.stdout.reconfigure(encoding='utf-8')

sys.path.insert(0, '.')

from memory_price_monitor.crawlers.playwright_zol_crawler import PlaywrightZOLCrawler
from memory_price_monitor.data.sqlite_database import SQLiteDatabaseManager
from memory_price_monitor.data.repository import PriceRepository
from memory_price_monitor.data.models import DataStandardizer
from memory_price_monitor.utils.logging import get_logger

# Import concurrent components
from memory_price_monitor.concurrent.controller import ConcurrentCrawlerController
from memory_price_monitor.concurrent.models import ConcurrentConfig

logger = get_logger(__name__)


class ProgressBar:
    """ç®€å•çš„è¿›åº¦æ¡æ˜¾ç¤ºå·¥å…·"""
    
    def __init__(self, total: int, width: int = 50, prefix: str = "Progress"):
        self.total = total
        self.width = width
        self.prefix = prefix
        self.current = 0
        self.start_time = time.time()
    
    def update(self, current: int, suffix: str = "") -> None:
        """æ›´æ–°è¿›åº¦æ¡"""
        self.current = current
        percent = (current / self.total) * 100 if self.total > 0 else 0
        filled_width = int(self.width * current // self.total) if self.total > 0 else 0
        bar = 'â–ˆ' * filled_width + 'â–‘' * (self.width - filled_width)
        
        # è®¡ç®—é€Ÿåº¦å’ŒETA
        elapsed = time.time() - self.start_time
        if elapsed > 0 and current > 0:
            rate = current / elapsed
            eta = (self.total - current) / rate if rate > 0 else 0
            eta_str = f"ETA: {eta:.0f}s" if eta > 0 else "ETA: --"
            rate_str = f"{rate:.1f}/s"
        else:
            eta_str = "ETA: --"
            rate_str = "--/s"
        
        # æ‰“å°è¿›åº¦æ¡
        print(f'\r{self.prefix}: |{bar}| {current}/{self.total} ({percent:.1f}%) {rate_str} {eta_str} {suffix}', 
              end='', flush=True)
        
        if current >= self.total:
            print()  # å®Œæˆæ—¶æ¢è¡Œ
    
    def finish(self, message: str = "Complete") -> None:
        """å®Œæˆè¿›åº¦æ¡"""
        self.update(self.total, message)


class SimpleProgressMonitor:
    """ç®€å•çš„è¿›åº¦ç›‘æ§å™¨ï¼Œç”¨äºåœ¨æ²¡æœ‰å®Œæ•´ç›‘æ§ç³»ç»Ÿæ—¶æ˜¾ç¤ºåŸºæœ¬è¿›åº¦"""
    
    def __init__(self, total_tasks: int, update_interval: float = 2.0):
        self.total_tasks = total_tasks
        self.update_interval = update_interval
        self.progress_bar = ProgressBar(total_tasks, prefix="æŠ“å–è¿›åº¦")
        self.last_update = time.time()
        self.completed = 0
        self.failed = 0
    
    def update_progress(self, completed: int, failed: int, extra_info: str = "") -> None:
        """æ›´æ–°è¿›åº¦"""
        current_time = time.time()
        if current_time - self.last_update >= self.update_interval or completed + failed >= self.total_tasks:
            self.completed = completed
            self.failed = failed
            total_processed = completed + failed
            
            suffix = f"âœ…{completed} âŒ{failed}"
            if extra_info:
                suffix += f" {extra_info}"
            
            self.progress_bar.update(total_processed, suffix)
            self.last_update = current_time
    
    def finish(self) -> None:
        """å®Œæˆç›‘æ§"""
        self.progress_bar.finish(f"å®Œæˆ: âœ…{self.completed} âŒ{self.failed}")


# ZOLå†…å­˜å“ç‰Œåˆ—è¡¨ï¼ˆä»é¡µé¢ä¸Šè·å–çš„ä¸»è¦å“ç‰Œï¼‰
ZOL_MEMORY_BRANDS = [
    'ä¸ƒå½©è™¹', 'è‹±ç¿è¾¾', 'å½±é©°', 'é˜¿æ–¯åŠ ç‰¹', 'ç§‘èµ‹',
    'æµ·ç›—èˆ¹', 'ç‘åŠ¿', 'èŠå¥‡', 'é‡‘æ³°å…‹', 'é‡‘é‚¦ç§‘æŠ€',
    'ç‰¹ç§‘èŠ¯', 'é‡‘å£«é¡¿', 'å¨åˆš', 'å®‡ç»', 'ä¸‰æ˜Ÿ',
    'å…‰å¨', 'ç°ä»£', 'åˆ›è§', 'åé“¨ç§‘æŠ€', 'é‡‘ç™¾è¾¾',
    'åšå¸', 'PNY', 'å›½æƒ ', 'æƒ æ™®', 'æ˜±è”',
    'ç¾å•†æµ·ç›—èˆ¹', 'é›·å…‹æ²™', 'Acerå®ç¢', 'è”æƒ³', 'ç–åˆ',
    'ä½°ç»´', 'é…·å…½', 'æµ·åŠ›å£«', 'å…ˆé”‹', 'ç‘¾å®‡',
    'é“­ç‘„', 'æ²ƒå­˜', 'é•¿åŸ', 'ç´«å…‰', 'è®°å¿†ç§‘æŠ€'
]


class ZOLDataCrawler:
    """ZOLæ•°æ®æŠ“å–å’Œå­˜å‚¨ç®¡ç†å™¨"""
    
    def __init__(self, db_path: str = "data/memory_price_monitor.db", concurrent_mode: bool = False):
        """åˆå§‹åŒ–çˆ¬è™«å’Œæ•°æ®åº“"""
        self.db_path = db_path
        self.concurrent_mode = concurrent_mode
        
        if not concurrent_mode:
            # å•çº¿ç¨‹æ¨¡å¼ï¼šä½¿ç”¨ä¼ ç»Ÿæ–¹å¼
            self.db_manager = SQLiteDatabaseManager(db_path)
            self.db_manager.initialize()
            self.repository = PriceRepository(self.db_manager)
            self.standardizer = DataStandardizer()
        
        # çˆ¬è™«é…ç½®
        self.crawler_config = {
            'headless': True,  # åå°è¿è¡Œ
            'browser_type': 'chromium',
            'max_pages': 1,  # æ¯ä¸ªå“ç‰ŒæŠ“å–1é¡µ
            'min_delay': 2.0,
            'max_delay': 4.0,
            'page_timeout': 30000
        }
    
    def crawl_all_brands_concurrent(self, brands: List[str] = None, max_workers: int = 4, 
                                  requests_per_second: float = 2.0, show_progress: bool = True) -> Dict[str, int]:
        """
        ä½¿ç”¨å¹¶å‘æ¨¡å¼æŠ“å–æ‰€æœ‰å“ç‰Œçš„å†…å­˜äº§å“
        
        Args:
            brands: å“ç‰Œåˆ—è¡¨ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤åˆ—è¡¨
            max_workers: æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°
            requests_per_second: æ¯ç§’è¯·æ±‚æ•°é™åˆ¶
            show_progress: æ˜¯å¦æ˜¾ç¤ºå®æ—¶è¿›åº¦
            
        Returns:
            æŠ“å–ç»“æœç»Ÿè®¡
        """
        if brands is None:
            brands = ZOL_MEMORY_BRANDS
        
        print("=" * 80)
        print("å¼€å§‹å¹¶å‘æŠ“å–ZOLå†…å­˜äº§å“æ•°æ®")
        print("=" * 80)
        print(f"ç›®æ ‡å“ç‰Œæ•°: {len(brands)}")
        print(f"å¹¶å‘çº¿ç¨‹æ•°: {max_workers}")
        print(f"è¯·æ±‚é¢‘ç‡: {requests_per_second} req/s")
        print(f"æ•°æ®åº“è·¯å¾„: {self.db_path}")
        print(f"å®æ—¶è¿›åº¦: {'å¯ç”¨' if show_progress else 'ç¦ç”¨'}")
        print()
        
        try:
            # åˆ›å»ºå¹¶å‘é…ç½®
            config = ConcurrentConfig(
                max_workers=max_workers,
                requests_per_second=requests_per_second,
                max_concurrent_requests=max_workers,
                retry_attempts=3,
                timeout_seconds=60,
                enable_task_stealing=True,
                enable_adaptive_rate=True,
                queue_timeout=10.0
            )
            
            # åˆ›å»ºå¹¶å‘æ§åˆ¶å™¨
            controller = ConcurrentCrawlerController(config)
            
            print("ğŸš€ å¯åŠ¨å¹¶å‘æŠ“å–...")
            start_time = time.time()
            
            # é…ç½®ç›‘æ§é€‰é¡¹
            additional_config = {
                'enable_monitoring': show_progress,
                'monitor_console_output': show_progress,
                'monitor_update_interval': 3.0,
                'monitor_console_interval': 8.0
            }
            
            # å¼€å§‹å¹¶å‘æŠ“å–ï¼ˆå¸¦è¿›åº¦ç›‘æ§ï¼‰
            result = controller.start_concurrent_crawling(brands, additional_config)
            
            end_time = time.time()
            execution_time = end_time - start_time
            
            # æ‰“å°ç»“æœ
            print("\n" + "=" * 80)
            print("å¹¶å‘æŠ“å–å®Œæˆ - æ€»ç»“")
            print("=" * 80)
            print(f"æ€»å“ç‰Œæ•°: {result.total_tasks}")
            print(f"æˆåŠŸä»»åŠ¡: {result.completed_tasks}")
            print(f"å¤±è´¥ä»»åŠ¡: {result.failed_tasks}")
            print(f"æˆåŠŸç‡: {result.get_success_rate():.1f}%")
            print(f"æ€»äº§å“æ•°: {result.total_products_found}")
            print(f"ä¿å­˜äº§å“æ•°: {result.total_products_saved}")
            print(f"æå–æ•ˆç‡: {result.get_efficiency():.1f}%")
            print(f"æ‰§è¡Œæ—¶é—´: {execution_time:.1f} ç§’")
            print(f"ååé‡: {result.get_throughput():.2f} äº§å“/ç§’")
            print()
            
            # æ‰“å°å·¥ä½œçº¿ç¨‹ç»Ÿè®¡
            print("å·¥ä½œçº¿ç¨‹ç»Ÿè®¡:")
            print("-" * 60)
            performance_report = controller.get_performance_report()
            worker_stats = performance_report.get('worker_statistics', {})
            
            for worker_id, stats in worker_stats.items():
                print(f"  {worker_id}: å®Œæˆ={stats.get('tasks_completed', 0)}, "
                      f"å¤±è´¥={stats.get('tasks_failed', 0)}, "
                      f"äº§å“={stats.get('products_saved', 0)}, "
                      f"å¹³å‡æ—¶é—´={stats.get('average_task_time', 0):.1f}s")
            
            # æ˜¾ç¤ºè¯¦ç»†æ€§èƒ½æŠ¥å‘Š
            if show_progress and performance_report:
                self._print_detailed_performance_report(performance_report)
            
            return {
                'total_tasks': result.total_tasks,
                'completed_tasks': result.completed_tasks,
                'failed_tasks': result.failed_tasks,
                'products_found': result.total_products_found,
                'products_saved': result.total_products_saved,
                'execution_time': execution_time,
                'success_rate': result.get_success_rate(),
                'throughput': result.get_throughput()
            }
            
        except Exception as e:
            print(f"âŒ å¹¶å‘æŠ“å–å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return {'error': str(e)}
    
    def crawl_all_brands(self, brands: List[str] = None) -> Dict[str, int]:
        """
        æŠ“å–æ‰€æœ‰å“ç‰Œçš„å†…å­˜äº§å“ï¼ˆå•çº¿ç¨‹æ¨¡å¼ï¼‰
        
        Args:
            brands: å“ç‰Œåˆ—è¡¨ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤åˆ—è¡¨
            
        Returns:
            æ¯ä¸ªå“ç‰ŒæŠ“å–çš„äº§å“æ•°é‡ç»Ÿè®¡
        """
        if brands is None:
            brands = ZOL_MEMORY_BRANDS
        
        print("=" * 80)
        print("å¼€å§‹æŠ“å–ZOLå†…å­˜äº§å“æ•°æ®ï¼ˆå•çº¿ç¨‹æ¨¡å¼ï¼‰")
        print("=" * 80)
        print(f"ç›®æ ‡å“ç‰Œæ•°: {len(brands)}")
        print(f"æ•°æ®åº“è·¯å¾„: {self.db_manager.database_path}")
        print()
        
        stats = {}
        total_products = 0
        total_saved = 0
        
        for i, brand in enumerate(brands, 1):
            print(f"\n[{i}/{len(brands)}] æ­£åœ¨æŠ“å–å“ç‰Œ: {brand}")
            print("-" * 60)
            
            try:
                # æŠ“å–è¯¥å“ç‰Œçš„äº§å“
                products = self._crawl_brand(brand)
                
                if products:
                    # ä¿å­˜åˆ°æ•°æ®åº“
                    saved_count = self._save_products(products)
                    
                    stats[brand] = {
                        'found': len(products),
                        'saved': saved_count
                    }
                    
                    total_products += len(products)
                    total_saved += saved_count
                    
                    print(f"âœ… {brand}: æ‰¾åˆ° {len(products)} ä¸ªäº§å“, ä¿å­˜ {saved_count} æ¡è®°å½•")
                else:
                    stats[brand] = {'found': 0, 'saved': 0}
                    print(f"âš ï¸  {brand}: æœªæ‰¾åˆ°äº§å“")
                
                # å“ç‰Œä¹‹é—´å»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡å¿«
                if i < len(brands):
                    delay = 3
                    print(f"â³ ç­‰å¾… {delay} ç§’åç»§ç»­...")
                    time.sleep(delay)
                    
            except Exception as e:
                print(f"âŒ {brand}: æŠ“å–å¤±è´¥ - {e}")
                stats[brand] = {'found': 0, 'saved': 0, 'error': str(e)}
                continue
        
        # æ‰“å°æ€»ç»“
        print("\n" + "=" * 80)
        print("æŠ“å–å®Œæˆ - æ€»ç»“")
        print("=" * 80)
        print(f"æ€»å“ç‰Œæ•°: {len(brands)}")
        print(f"æ€»äº§å“æ•°: {total_products}")
        print(f"æ€»ä¿å­˜æ•°: {total_saved}")
        print()
        
        # æ‰“å°è¯¦ç»†ç»Ÿè®¡
        print("å“ç‰Œè¯¦ç»†ç»Ÿè®¡:")
        print("-" * 80)
        for brand, stat in stats.items():
            if stat['found'] > 0:
                print(f"  {brand:<15} æ‰¾åˆ°: {stat['found']:>3}  ä¿å­˜: {stat['saved']:>3}")
        
        return stats
    
    def _crawl_brand(self, brand: str) -> List:
        """
        æŠ“å–ç‰¹å®šå“ç‰Œçš„äº§å“
        
        Args:
            brand: å“ç‰Œåç§°
            
        Returns:
            äº§å“åˆ—è¡¨
        """
        # åˆ›å»ºçˆ¬è™«å®ä¾‹
        config = self.crawler_config.copy()
        config['search_keywords'] = [f'{brand} å†…å­˜']
        
        crawler = PlaywrightZOLCrawler(config=config)
        
        try:
            # ä½¿ç”¨åˆ†ç±»é¡µé¢æ–¹æ³•æŠ“å–
            products = crawler._fetch_from_category_page()
            
            # è¿‡æ»¤å‡ºè¯¥å“ç‰Œçš„äº§å“
            brand_products = []
            for product in products:
                product_brand = product.raw_data.get('brand', '').lower()
                if brand.lower() in product_brand or product_brand in brand.lower():
                    brand_products.append(product)
            
            return brand_products
            
        finally:
            crawler.cleanup()
    
    def _save_products(self, products: List) -> int:
        """
        ä¿å­˜äº§å“åˆ°æ•°æ®åº“
        
        Args:
            products: äº§å“åˆ—è¡¨
            
        Returns:
            æˆåŠŸä¿å­˜çš„æ•°é‡
        """
        saved_count = 0
        
        for product in products:
            try:
                # æ ‡å‡†åŒ–äº§å“æ•°æ®
                standardized = self.standardizer.standardize(product, 'zol_playwright')
                
                # ä¿å­˜åˆ°æ•°æ®åº“
                self.repository.save_price_record(standardized)
                saved_count += 1
                
            except Exception as e:
                logger.warning(f"ä¿å­˜äº§å“å¤±è´¥ {product.product_id}: {e}")
                continue
        
        return saved_count
    
    def crawl_category_page(self) -> Dict[str, int]:
        """
        ç›´æ¥æŠ“å–åˆ†ç±»é¡µé¢çš„æ‰€æœ‰äº§å“ï¼ˆä¸æŒ‰å“ç‰Œè¿‡æ»¤ï¼‰
        
        Returns:
            æŠ“å–ç»Ÿè®¡
        """
        print("=" * 80)
        print("æŠ“å–ZOLå†…å­˜åˆ†ç±»é¡µé¢")
        print("=" * 80)
        
        config = self.crawler_config.copy()
        config['search_keywords'] = ['å†…å­˜æ¡']
        
        crawler = PlaywrightZOLCrawler(config=config)
        
        try:
            print("ğŸš€ å¼€å§‹æŠ“å–...")
            products = crawler._fetch_from_category_page()
            
            print(f"ğŸ“Š æ‰¾åˆ° {len(products)} ä¸ªäº§å“")
            
            if products:
                print("ğŸ’¾ ä¿å­˜åˆ°æ•°æ®åº“...")
                saved_count = self._save_products(products)
                
                print(f"âœ… æˆåŠŸä¿å­˜ {saved_count} æ¡è®°å½•")
                
                return {
                    'found': len(products),
                    'saved': saved_count
                }
            else:
                print("âš ï¸  æœªæ‰¾åˆ°äº§å“")
                return {'found': 0, 'saved': 0}
                
        finally:
            crawler.cleanup()
    
    def _print_detailed_performance_report(self, performance_report: Dict[str, Any]) -> None:
        """
        æ‰“å°è¯¦ç»†çš„æ€§èƒ½æŠ¥å‘Š
        
        Args:
            performance_report: æ€§èƒ½æŠ¥å‘Šæ•°æ®
        """
        print("\n" + "=" * 80)
        print("è¯¦ç»†æ€§èƒ½æŠ¥å‘Š")
        print("=" * 80)
        
        # ç»„ä»¶ç»Ÿè®¡
        component_stats = performance_report.get('component_statistics', {})
        
        # è°ƒåº¦å™¨ç»Ÿè®¡
        scheduler_stats = component_stats.get('scheduler', {})
        if scheduler_stats:
            print("ğŸ“‹ ä»»åŠ¡è°ƒåº¦å™¨:")
            print(f"  é˜Ÿåˆ—å¤§å°: {scheduler_stats.get('queue_size', 0)}")
            print(f"  å¾…å¤„ç†ä»»åŠ¡: {scheduler_stats.get('pending_tasks', 0)}")
            print(f"  ä»»åŠ¡åˆ†é…æ¬¡æ•°: {scheduler_stats.get('tasks_assigned', 0)}")
            print()
        
        # çº¿ç¨‹æ± ç»Ÿè®¡
        thread_pool_stats = component_stats.get('thread_pool', {})
        if thread_pool_stats:
            print("ğŸ§µ çº¿ç¨‹æ± :")
            print(f"  æ´»è·ƒçº¿ç¨‹: {thread_pool_stats.get('active_workers', 0)}")
            print(f"  å¥åº·çº¿ç¨‹: {thread_pool_stats.get('healthy_workers', 0)}")
            print(f"  æ€»çº¿ç¨‹æ•°: {thread_pool_stats.get('total_workers', 0)}")
            print()
        
        # é€Ÿç‡æ§åˆ¶å™¨ç»Ÿè®¡
        rate_stats = component_stats.get('rate_controller', {})
        if rate_stats:
            print("âš¡ é€Ÿç‡æ§åˆ¶:")
            print(f"  å½“å‰é€Ÿç‡: {rate_stats.get('current_rate', 0):.2f} req/s")
            print(f"  é€Ÿç‡é™åˆ¶: {rate_stats.get('requests_per_second_limit', 0):.2f} req/s")
            print(f"  æ´»è·ƒè¯·æ±‚: {rate_stats.get('active_requests', 0)}")
            print(f"  ç­‰å¾…æ¬¡æ•°: {rate_stats.get('wait_count', 0)}")
            print()
        
        # æ•°æ®åº“ç»Ÿè®¡
        repo_stats = component_stats.get('repository', {})
        if repo_stats:
            print("ğŸ’¾ æ•°æ®åº“:")
            print(f"  è¿æ¥æ± å¤§å°: {repo_stats.get('connection_pool_size', 0)}")
            print(f"  æ´»è·ƒè¿æ¥: {repo_stats.get('active_connections', 0)}")
            print(f"  æ‰¹é‡å†™å…¥æ¬¡æ•°: {repo_stats.get('batch_writes', 0)}")
            print(f"  å†™å…¥å†²çªæ¬¡æ•°: {repo_stats.get('write_conflicts', 0)}")
            print()
        
        # æ‰§è¡Œè¯¦æƒ…
        execution_details = performance_report.get('execution_details', {})
        if execution_details:
            print("â±ï¸  æ‰§è¡Œè¯¦æƒ…:")
            start_time = execution_details.get('start_time')
            end_time = execution_details.get('end_time')
            if start_time:
                print(f"  å¼€å§‹æ—¶é—´: {start_time}")
            if end_time:
                print(f"  ç»“æŸæ—¶é—´: {end_time}")
            print(f"  æ€»æ‰§è¡Œæ—¶é—´: {execution_details.get('total_execution_time', 0):.2f}s")
            print()
    
    def show_database_stats(self):
        """æ˜¾ç¤ºæ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯"""
        print("\n" + "=" * 80)
        print("æ•°æ®åº“ç»Ÿè®¡")
        print("=" * 80)
        
        try:
            if self.concurrent_mode:
                # å¹¶å‘æ¨¡å¼ä¸‹ï¼Œéœ€è¦ä¸´æ—¶åˆ›å»ºrepositoryæ¥è·å–ç»Ÿè®¡
                db_manager = SQLiteDatabaseManager(self.db_path)
                db_manager.initialize()
                repository = PriceRepository(db_manager)
                stats = repository.get_database_stats()
            else:
                stats = self.repository.get_database_stats()
            
            print(f"æ€»äº§å“æ•°: {stats.get('total_products', 0)}")
            print(f"æ€»ä»·æ ¼è®°å½•æ•°: {stats.get('total_price_records', 0)}")
            
            if 'earliest_record' in stats:
                print(f"æœ€æ—©è®°å½•: {stats['earliest_record']}")
                print(f"æœ€æ–°è®°å½•: {stats['latest_record']}")
            
            print("\næŒ‰æ¥æºç»Ÿè®¡:")
            for source, count in stats.get('products_by_source', {}).items():
                print(f"  {source}: {count}")
            
            print("\nçƒ­é—¨å“ç‰Œ (Top 10):")
            for brand, count in stats.get('top_brands', {}).items():
                print(f"  {brand}: {count}")
                
        except Exception as e:
            print(f"âŒ è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")


def parse_arguments():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description="ZOLå†…å­˜ä»·æ ¼æ•°æ®æŠ“å–ç³»ç»Ÿ",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  python crawl_and_save_zol_data.py                    # äº¤äº’æ¨¡å¼
  python crawl_and_save_zol_data.py --mode category    # å¿«é€ŸæŠ“å–åˆ†ç±»é¡µé¢
  python crawl_and_save_zol_data.py --mode brands      # æŒ‰å“ç‰ŒæŠ“å–ï¼ˆå•çº¿ç¨‹ï¼‰
  python crawl_and_save_zol_data.py --mode concurrent  # å¹¶å‘æŠ“å–
  python crawl_and_save_zol_data.py --mode stats       # åªæ˜¾ç¤ºç»Ÿè®¡
  
å¹¶å‘æ¨¡å¼é€‰é¡¹:
  python crawl_and_save_zol_data.py --mode concurrent --workers 4 --rate 2.0
  python crawl_and_save_zol_data.py --mode concurrent --no-progress --quiet
  python crawl_and_save_zol_data.py --mode concurrent --timeout 120 --retries 5
        """
    )
    
    parser.add_argument(
        '--mode', 
        choices=['category', 'brands', 'concurrent', 'stats'],
        help='æŠ“å–æ¨¡å¼ï¼šcategory=åˆ†ç±»é¡µé¢, brands=æŒ‰å“ç‰Œ, concurrent=å¹¶å‘, stats=ç»Ÿè®¡'
    )
    
    # å¹¶å‘ç›¸å…³å‚æ•°
    concurrent_group = parser.add_argument_group('å¹¶å‘æ¨¡å¼å‚æ•°')
    concurrent_group.add_argument(
        '--workers', 
        type=int, 
        default=4,
        help='å¹¶å‘æ¨¡å¼ä¸‹çš„å·¥ä½œçº¿ç¨‹æ•° (é»˜è®¤: 4, èŒƒå›´: 1-8)'
    )
    
    concurrent_group.add_argument(
        '--rate', 
        type=float, 
        default=2.0,
        help='å¹¶å‘æ¨¡å¼ä¸‹çš„è¯·æ±‚é¢‘ç‡ req/s (é»˜è®¤: 2.0, èŒƒå›´: 0.5-5.0)'
    )
    
    concurrent_group.add_argument(
        '--timeout', 
        type=int, 
        default=60,
        help='ä»»åŠ¡è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ (é»˜è®¤: 60, èŒƒå›´: 30-300)'
    )
    
    concurrent_group.add_argument(
        '--retries', 
        type=int, 
        default=3,
        help='å¤±è´¥é‡è¯•æ¬¡æ•° (é»˜è®¤: 3, èŒƒå›´: 1-10)'
    )
    
    # æ˜¾ç¤ºå’Œè¾“å‡ºå‚æ•°
    display_group = parser.add_argument_group('æ˜¾ç¤ºå’Œè¾“å‡ºå‚æ•°')
    display_group.add_argument(
        '--no-progress', 
        action='store_true',
        help='ç¦ç”¨å®æ—¶è¿›åº¦æ˜¾ç¤ºï¼ˆå¹¶å‘æ¨¡å¼ï¼‰'
    )
    
    display_group.add_argument(
        '--quiet', 
        action='store_true',
        help='é™é»˜æ¨¡å¼ï¼šå‡å°‘è¾“å‡ºä¿¡æ¯'
    )
    
    display_group.add_argument(
        '--verbose', 
        action='store_true',
        help='è¯¦ç»†æ¨¡å¼ï¼šæ˜¾ç¤ºæ›´å¤šè°ƒè¯•ä¿¡æ¯'
    )
    
    # é€šç”¨å‚æ•°
    parser.add_argument(
        '--test', 
        action='store_true',
        help='æµ‹è¯•æ¨¡å¼ï¼šåªæŠ“å–å‰5ä¸ªå“ç‰Œ'
    )
    
    parser.add_argument(
        '--brands',
        nargs='+',
        help='æŒ‡å®šè¦æŠ“å–çš„å“ç‰Œåˆ—è¡¨ï¼ˆç©ºæ ¼åˆ†éš”ï¼‰'
    )
    
    parser.add_argument(
        '--db-path', 
        default="data/memory_price_monitor.db",
        help='æ•°æ®åº“æ–‡ä»¶è·¯å¾„ (é»˜è®¤: data/memory_price_monitor.db)'
    )
    
    return parser.parse_args()


def main():
    """ä¸»å‡½æ•°"""
    args = parse_arguments()
    
    # å‚æ•°éªŒè¯å’Œè°ƒæ•´
    if args.workers:
        args.workers = max(1, min(args.workers, 8))  # é™åˆ¶åœ¨1-8èŒƒå›´å†…
    
    if args.rate:
        args.rate = max(0.5, min(args.rate, 5.0))  # é™åˆ¶åœ¨0.5-5.0èŒƒå›´å†…
    
    if args.timeout:
        args.timeout = max(30, min(args.timeout, 300))  # é™åˆ¶åœ¨30-300ç§’èŒƒå›´å†…
    
    if args.retries:
        args.retries = max(1, min(args.retries, 10))  # é™åˆ¶åœ¨1-10æ¬¡èŒƒå›´å†…
    
    # è®¾ç½®æ—¥å¿—çº§åˆ«
    if args.verbose:
        import logging
        logging.getLogger().setLevel(logging.DEBUG)
    elif args.quiet:
        import logging
        logging.getLogger().setLevel(logging.WARNING)
    
    if not args.quiet:
        print("ğŸ§ª ZOLå†…å­˜ä»·æ ¼æ•°æ®æŠ“å–ç³»ç»Ÿ")
        print(f"ğŸ“… å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print()
    
    # åˆ›å»ºçˆ¬è™«å®ä¾‹
    concurrent_mode = (args.mode == 'concurrent')
    crawler = ZOLDataCrawler(db_path=args.db_path, concurrent_mode=concurrent_mode)
    
    try:
        if args.mode:
            # å‘½ä»¤è¡Œæ¨¡å¼
            if args.mode == 'category':
                if not args.quiet:
                    print("ğŸ“‹ æ¨¡å¼: æŠ“å–åˆ†ç±»é¡µé¢")
                stats = crawler.crawl_category_page()
                
            elif args.mode == 'brands':
                if not args.quiet:
                    print("ğŸ·ï¸  æ¨¡å¼: æŒ‰å“ç‰ŒæŠ“å–ï¼ˆå•çº¿ç¨‹ï¼‰")
                
                # ç¡®å®šå“ç‰Œåˆ—è¡¨
                if args.brands:
                    brands = args.brands
                    if not args.quiet:
                        print(f"æŒ‡å®šå“ç‰Œ: {', '.join(brands)}")
                elif args.test:
                    brands = ZOL_MEMORY_BRANDS[:5]
                    if not args.quiet:
                        print(f"æµ‹è¯•æ¨¡å¼å“ç‰Œ: {', '.join(brands)}")
                else:
                    brands = ZOL_MEMORY_BRANDS
                    if not args.quiet:
                        print(f"å…¨éƒ¨å“ç‰Œæ•°é‡: {len(brands)}")
                
                stats = crawler.crawl_all_brands(brands)
                
            elif args.mode == 'concurrent':
                if not args.quiet:
                    print("âš¡ æ¨¡å¼: å¹¶å‘æŠ“å–")
                
                # ç¡®å®šå“ç‰Œåˆ—è¡¨
                if args.brands:
                    brands = args.brands
                    if not args.quiet:
                        print(f"æŒ‡å®šå“ç‰Œ: {', '.join(brands)}")
                elif args.test:
                    brands = ZOL_MEMORY_BRANDS[:5]
                    if not args.quiet:
                        print(f"æµ‹è¯•æ¨¡å¼å“ç‰Œ: {', '.join(brands)}")
                else:
                    brands = ZOL_MEMORY_BRANDS
                    if not args.quiet:
                        print(f"å…¨éƒ¨å“ç‰Œæ•°é‡: {len(brands)}")
                
                if not args.quiet:
                    print(f"å·¥ä½œçº¿ç¨‹: {args.workers}")
                    print(f"è¯·æ±‚é¢‘ç‡: {args.rate} req/s")
                    print(f"ä»»åŠ¡è¶…æ—¶: {args.timeout}s")
                    print(f"é‡è¯•æ¬¡æ•°: {args.retries}")
                    print(f"å®æ—¶è¿›åº¦: {'ç¦ç”¨' if args.no_progress else 'å¯ç”¨'}")
                
                # åˆ›å»ºå¢å¼ºçš„å¹¶å‘é…ç½®
                enhanced_config = {
                    'timeout_seconds': args.timeout,
                    'retry_attempts': args.retries,
                    'show_progress': not args.no_progress,
                    'quiet_mode': args.quiet
                }
                
                stats = crawler.crawl_all_brands_concurrent(
                    brands=brands,
                    max_workers=args.workers,
                    requests_per_second=args.rate,
                    show_progress=not args.no_progress
                )
                
            elif args.mode == 'stats':
                if not args.quiet:
                    print("ğŸ“Š æ¨¡å¼: æ˜¾ç¤ºç»Ÿè®¡")
                crawler.show_database_stats()
                return 0
        
        else:
            # äº¤äº’æ¨¡å¼
            print("è¯·é€‰æ‹©æŠ“å–æ¨¡å¼:")
            print("1. æŠ“å–åˆ†ç±»é¡µé¢ï¼ˆå¿«é€Ÿï¼Œæ¨èï¼‰")
            print("2. æŒ‰å“ç‰ŒæŠ“å–ï¼ˆå…¨é¢ï¼Œå•çº¿ç¨‹ï¼‰")
            print("3. å¹¶å‘æŠ“å–ï¼ˆå¿«é€Ÿï¼Œå¤šçº¿ç¨‹ï¼‰")
            print("4. åªæ˜¾ç¤ºæ•°æ®åº“ç»Ÿè®¡")
            
            choice = input("\nè¯·è¾“å…¥é€‰é¡¹ (1/2/3/4): ").strip()
            
            if choice == '1':
                # å¿«é€Ÿæ¨¡å¼ï¼šç›´æ¥æŠ“å–åˆ†ç±»é¡µé¢
                stats = crawler.crawl_category_page()
                
            elif choice == '2':
                # å•çº¿ç¨‹æ¨¡å¼ï¼šæŒ‰å“ç‰ŒæŠ“å–
                print("\næ˜¯å¦åªæŠ“å–éƒ¨åˆ†å“ç‰Œè¿›è¡Œæµ‹è¯•ï¼Ÿ")
                test_mode = input("è¾“å…¥ 'y' è¿›è¡Œæµ‹è¯•ï¼ˆåªæŠ“å–å‰5ä¸ªå“ç‰Œï¼‰ï¼Œå¦åˆ™æŠ“å–å…¨éƒ¨: ").strip().lower()
                
                if test_mode == 'y':
                    brands = ZOL_MEMORY_BRANDS[:5]
                    print(f"\næµ‹è¯•æ¨¡å¼ï¼šåªæŠ“å– {len(brands)} ä¸ªå“ç‰Œ")
                else:
                    brands = ZOL_MEMORY_BRANDS
                    print(f"\nå®Œæ•´æ¨¡å¼ï¼šæŠ“å– {len(brands)} ä¸ªå“ç‰Œ")
                
                stats = crawler.crawl_all_brands(brands)
                
            elif choice == '3':
                # å¹¶å‘æ¨¡å¼
                print("\né…ç½®å¹¶å‘å‚æ•°:")
                
                # é€‰æ‹©å“ç‰ŒèŒƒå›´
                test_mode = input("æ˜¯å¦æµ‹è¯•æ¨¡å¼ï¼ˆåªæŠ“å–å‰5ä¸ªå“ç‰Œï¼‰ï¼Ÿ(y/N): ").strip().lower()
                brands = ZOL_MEMORY_BRANDS[:5] if test_mode == 'y' else ZOL_MEMORY_BRANDS
                
                # é…ç½®å¹¶å‘å‚æ•°
                workers_input = input(f"å·¥ä½œçº¿ç¨‹æ•° (1-8, é»˜è®¤4): ").strip()
                workers = int(workers_input) if workers_input.isdigit() and 1 <= int(workers_input) <= 8 else 4
                
                rate_input = input(f"è¯·æ±‚é¢‘ç‡ req/s (0.5-5.0, é»˜è®¤2.0): ").strip()
                try:
                    rate = float(rate_input) if rate_input else 2.0
                    rate = max(0.5, min(rate, 5.0))  # é™åˆ¶èŒƒå›´
                except ValueError:
                    rate = 2.0
                
                # è¿›åº¦æ˜¾ç¤ºé€‰é¡¹
                progress_input = input("æ˜¾ç¤ºå®æ—¶è¿›åº¦ï¼Ÿ(Y/n): ").strip().lower()
                show_progress = progress_input != 'n'
                
                print(f"\nå¹¶å‘é…ç½®: {len(brands)} å“ç‰Œ, {workers} çº¿ç¨‹, {rate} req/s, è¿›åº¦={'å¯ç”¨' if show_progress else 'ç¦ç”¨'}")
                
                stats = crawler.crawl_all_brands_concurrent(
                    brands=brands,
                    max_workers=workers,
                    requests_per_second=rate,
                    show_progress=show_progress
                )
                
            elif choice == '4':
                # åªæ˜¾ç¤ºç»Ÿè®¡
                crawler.show_database_stats()
                return 0
                
            else:
                print("âŒ æ— æ•ˆé€‰é¡¹")
                return 1
        
        # æ˜¾ç¤ºæ•°æ®åº“ç»Ÿè®¡
        if not args.quiet:
            crawler.show_database_stats()
        
        if not args.quiet:
            print("\n" + "=" * 80)
            print("âœ… ä»»åŠ¡å®Œæˆ!")
            print("=" * 80)
        
        return 0
        
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­")
        return 1
    except Exception as e:
        print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()
        return 1


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)
